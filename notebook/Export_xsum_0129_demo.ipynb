{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import PegasusTokenizer\n",
    "from transformers import PegasusForConditionalGeneration\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model_name=\"google/pegasus-xsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_pegasus_model = PegasusForConditionalGeneration.from_pretrained(ori_model_name)\n",
    "ori_tokenizer = PegasusTokenizer.from_pretrained(ori_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_encoder(model, args, exported_model_path):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = torch.onnx._export(model,\n",
    "                           args,\n",
    "                           exported_model_path,\n",
    "                           export_params=True,\n",
    "                           opset_version=14,\n",
    "                           input_names=['input_ids'],\n",
    "                           output_names=['hidden_states'],\n",
    "                           dynamic_axes={\n",
    "                               'input_ids': {0:'batch', 1: 'sequence'},\n",
    "                               'hidden_states': {0:'batch', 1: 'sequence'},\n",
    "                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithLMHead(torch.nn.Module):\n",
    "    def __init__(self, decoder, lm_head, final_logits_bias):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "        self.final_logits_bias = final_logits_bias\n",
    "        \n",
    "    def forward(self, input_ids, encoder_hidden_states):\n",
    "        outputs = self.decoder(input_ids=input_ids,\n",
    "                               attention_mask=None,\n",
    "                               encoder_hidden_states=encoder_hidden_states)\n",
    "        logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        log_softmaxed = F.log_softmax(next_token_logits, 1)\n",
    "        topk = torch.topk(log_softmaxed, 5, largest=True)\n",
    "        return topk.values, topk.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_decoder(model, decoder_inputs, encoded, model_path):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = torch.onnx.export(model,\n",
    "                  (decoder_inputs, encoded),\n",
    "                  output_decoder_path,\n",
    "                  export_params=True,\n",
    "                  opset_version=12,\n",
    "                  input_names=['input_ids', 'encoder_hidden_states'],\n",
    "                  output_names=['log_softmax', 'indices'],\n",
    "                  dynamic_axes={\n",
    "                          'input_ids': {0:'batch', 1: 'sequence'},\n",
    "                          'encoder_hidden_states': {0:'batch', 1: 'sequence'},\n",
    "                          'log_softmax': {0:'batch'},\n",
    "                          'indices': {0:'batch'},\n",
    "                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encoder_path = \"./onnx_output/encoder_xsum_0129.onnx\"\n",
    "output_decoder_path = \"./onnx_output/decoder_lm_xsum_0129.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_text = \"\"\"\n",
    "I have been going over my folder.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_encoder_and_decoder(tokenizer, model, export_text, output_encoder_path, output_decoder_path):\n",
    "    export_input = tokenizer(export_text, return_tensors='pt')\n",
    "    export_encoder(model.model.encoder, export_input['input_ids'], output_encoder_path)\n",
    "    decoder_lm_head = DecoderWithLMHead(model.model.decoder, model.lm_head, model.final_logits_bias)\n",
    "    export_decoder(decoder_lm_head, export_input['input_ids'], model.model.encoder(input_ids=export_input['input_ids']).last_hidden_state,output_decoder_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shu.cai/pegasus_venv/lib/python3.9/site-packages/transformers/models/pegasus/modeling_pegasus.py:867: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/shu.cai/pegasus_venv/lib/python3.9/site-packages/transformers/models/pegasus/modeling_pegasus.py:240: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n"
     ]
    }
   ],
   "source": [
    "export_encoder_and_decoder(ori_tokenizer, ori_pegasus_model, export_text, output_encoder_path, output_decoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_options = ort.SessionOptions()\n",
    "sess_options.intra_op_num_threads = 4\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "EP_list = ['CUDAExecutionProvider']\n",
    "ori_encoder_session = ort.InferenceSession(output_encoder_path, sess_options, providers=EP_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 76])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"I have already taken classes in NLP, ML (both intro and grad level), and algorithms. \n",
    "I was even the teaching assistant for algorithms. \n",
    "I even was able to combine all of these in a self-project \n",
    "where I built a neural machine translation model capable of going from Shakespearean to modern English, \n",
    "which was able to make it all the way to the top of Hacker News. \n",
    "\"\"\"\n",
    "test_input=ori_tokenizer([text], return_tensors='pt')\n",
    "test_input[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch encoder result\n",
    "with torch.no_grad():\n",
    "    pt_encoder_hidden_state = ori_pegasus_model.model.encoder(input_ids=test_input['input_ids']).last_hidden_state\n",
    "# onnx encoder_result\n",
    "encoder_output_ori_onnx = ori_encoder_session.run(None, {'input_ids':test_input[\"input_ids\"].numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(pt_encoder_hidden_state.numpy(), encoder_output_ori_onnx[0], atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate decoder result using greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prob_index(decoder, lm_head, decoder_input_ids, encoder_hidden_states, final_logits_bias, output_topk):\n",
    "    outputs = decoder(input_ids=decoder_input_ids,\n",
    "                      attention_mask=None,\n",
    "                      encoder_hidden_states=encoder_hidden_states)\n",
    "    logits = lm_head(outputs[0]) + final_logits_bias\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    best_token_index = torch.argmax(next_token_logits, 1)\n",
    "    if output_topk:\n",
    "        log_softmaxed = F.log_softmax(next_token_logits, 1)\n",
    "        topk = torch.topk(log_softmaxed, 5, largest=True)\n",
    "        return (topk, best_token_index)\n",
    "    else:\n",
    "        return (next_token_logits, best_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_pytorch(decoder, lm_head, decoder_input_ids, encoder_hidden_states, final_logits_bias, output_topk):\n",
    "    decoder_input_cur = decoder_input_ids\n",
    "    next_token_logits_array = []\n",
    "    while True:\n",
    "        (next_token_logits, best_next) = get_top_prob_index(decoder, lm_head, decoder_input_cur, encoder_hidden_states, final_logits_bias, output_topk)\n",
    "        next_token_logits_array.append(next_token_logits)    \n",
    "        decoder_input_cur = torch.cat([decoder_input_cur, best_next.unsqueeze(1)], dim=-1)\n",
    "        if best_next == 1:\n",
    "            break\n",
    "    return (decoder_input_cur, next_token_logits_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = torch.tensor([[0]]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    (summarization_id_pt, next_token_pt) = get_results_from_pytorch(ori_pegasus_model.model.decoder, ori_pegasus_model.lm_head, \\\n",
    "                                                                    decoder_inputs, pt_encoder_hidden_state,  ori_pegasus_model.final_logits_bias, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(next_token_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[-1.0071, -1.7436, -2.3653, -3.0019, -3.2337]]),\n",
       "indices=tensor([[346, 131, 133, 245, 123]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_pt[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onnx results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_decoder_session = ort.InferenceSession(output_decoder_path, sess_options, providers=EP_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decoder_output = ori_decoder_session.run(None, {'input_ids': decoder_inputs.numpy(), \"encoder_hidden_states\": encoder_output_ori_onnx[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.65140724, -2.956626  , -3.3299437 , -3.4845095 , -4.494252  ]],\n",
       "       dtype=float32),\n",
       " array([[ 125,  600, 8087,  240,  398]], dtype=int64)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization_ids_onnx(encoder_output, decoder_session, init_decoder_inputs, max_length):\n",
    "    decoder_outputs = init_decoder_inputs\n",
    "    next_token_info_array = []\n",
    "    current_length = 1\n",
    "    while current_length < max_length:\n",
    "        onnx_decoder_outputs = decoder_session.run(None, {'input_ids': decoder_outputs, \"encoder_hidden_states\": encoder_output[0]})\n",
    "        next_token_info_array.append(onnx_decoder_outputs)\n",
    "        next_tokens = np.asarray([onnx_decoder_outputs[1][0][0]])\n",
    "        decoder_outputs = np.concatenate([decoder_outputs, next_tokens[:, None]], axis=-1)\n",
    "        if next_tokens[0] == 1:\n",
    "            break\n",
    "        current_length+=1\n",
    "    return (decoder_outputs, next_token_info_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "(summarization_id_onnx, next_token_onnx) = \\\n",
    "get_summarization_ids_onnx(encoder_output_ori_onnx, ori_decoder_session, decoder_inputs.numpy(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(summarization_id_onnx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(next_token_onnx) == len(next_token_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All decoder result close\n"
     ]
    }
   ],
   "source": [
    "not_close=False\n",
    "for i in range(len(next_token_onnx)):\n",
    "    if not np.allclose(next_token_pt[i][0].numpy(), next_token_onnx[i][0], atol=1e-4):\n",
    "        not_close=True\n",
    "if not_close:\n",
    "    print(\"Not all decoder result close\")\n",
    "else:\n",
    "    print(\"All decoder result close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoded_text(tokenizer, summarized_id):\n",
    "    return [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summarized_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_decoded_text(ori_tokenizer, summarization_id_pt) == get_decoded_text(ori_tokenizer, summarization_id_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a junior at the University of California, Berkeley, and am interested in Natural Language Processing (NLP).']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_decoded_text(ori_tokenizer, summarization_id_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pegasus_venv",
   "language": "python",
   "name": "pegasus_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
